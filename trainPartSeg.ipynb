{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from dataset import ShapeNetDataset\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_data(dataset,data_type,batch_size):\n",
    "    pcds,labels,fns = dataset.get_data(data_type,batch_size)\n",
    "    return pcds,labels\n",
    "    \n",
    "\n",
    "def compute_mIoU(preds,target):\n",
    "    preds = preds.cpu().detach().numpy()\n",
    "    target = target.cpu().detach().numpy()\n",
    "    N_of_parts = np.max(target)\n",
    "    N_of_pts = np.shape(target)[0]\n",
    "    total_IoU = 0.\n",
    "    for i in range(N_of_parts):\n",
    "        U = [False]*N_of_pts\n",
    "        I = [False]*N_of_pts\n",
    "        for j in range(N_of_pts):\n",
    "            if target[j] == i and preds[j] == i:\n",
    "                I[j] = True\n",
    "            if target[j] == i or preds[j] == i:\n",
    "                U[j] = True\n",
    "        if sum(U) == 0: \n",
    "            total_IoU += 1\n",
    "        else:\n",
    "            total_IoU += sum(I)/sum(U)\n",
    "    return total_IoU/N_of_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hyperparameters\n",
    "N_CLASSES = 16\n",
    "EPOCHS = 1000#2000\n",
    "BATCH_SIZE = 32\n",
    "INIT_LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "LR_STEP = 20\n",
    "SCHEDULER_GAMMA = 0.5\n",
    "VAL_EVERY = 1\n",
    "REG_WEIGHT = 0.001\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_ID = 0\n",
    "ShapeNetData = ShapeNetDataset('datasets/ShapeNet/',class_ID)\n",
    "N_of_parts = ShapeNetData.get_N_parts(class_ID)\n",
    "net = PointNetDenseClassification(N_of_parts).to(device)\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=INIT_LR)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP, gamma=SCHEDULER_GAMMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training loop\n",
    "train_losses = []\n",
    "train_iterations = []\n",
    "val_losses = []\n",
    "val_iterations = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    net.eval()#to allow for batch of 1\n",
    "    #load the batch of data\n",
    "    all_data,all_labels = sample_data(ShapeNetData,'train',BATCH_SIZE)\n",
    "    mIoU_batch = 0.\n",
    "    for data,labels in zip(all_data,all_labels):\n",
    "        data = torch.from_numpy(np.expand_dims(np.array(data),0)).float().to(device)\n",
    "        labels = torch.from_numpy(np.expand_dims(np.array(labels),0)).to(device)\n",
    "        \n",
    "        #compute the loss\n",
    "        preds,M2 = net(data)\n",
    "        loss = criterion(preds[0,:,:],labels[0,:])\n",
    "    \n",
    "        #add transformation matrix regularization loss\n",
    "        I = torch.eye(64).unsqueeze(0).to(device)\n",
    "        loss2 = torch.mean(torch.norm(torch.bmm(M2,M2.transpose(2,1)) - I, dim=(1,2)))    \n",
    "        loss += REG_WEIGHT*loss2\n",
    "        \n",
    "        mIoU_one_part = compute_mIoU(torch.max(preds[0,:,:],dim = 1).values,labels[0,:])\n",
    "        mIoU_batch +=mIoU_one_part \n",
    "        \n",
    "    train_losses.append(loss.detach().cpu())\n",
    "    train_iterations.append(epoch)\n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"epoch:\",epoch,\"mIoU:\",mIoU_batch/BATCH_SIZE)\n",
    "    if epoch%VAL_EVERY == 0:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            #load the batch of eval data (batch size couldn't be too big)\n",
    "            all_data,all_labels = sample_data(ShapeNetData,'val',BATCH_SIZE)\n",
    "            for data,labels in zip(all_data,all_labels):\n",
    "                data = torch.from_numpy(np.expand_dims(np.array(data),0)).float().to(device)\n",
    "                labels = torch.from_numpy(np.expand_dims(np.array(labels),0)).to(device)\n",
    "\n",
    "                #compute the loss\n",
    "                preds,M2 = net(data)\n",
    "                val_loss = criterion(preds[0,:,:],labels[0,:])\n",
    "\n",
    "                #add transformation matrix regularization loss\n",
    "                I = torch.eye(64).unsqueeze(0).to(device)\n",
    "                loss2 = torch.mean(torch.norm(torch.bmm(M2,M2.transpose(2,1)) - I, dim=(1,2)))    \n",
    "                val_loss += REG_WEIGHT*loss2\n",
    "\n",
    "            val_losses.append(val_loss.detach().cpu())\n",
    "            val_iterations.append(epoch)\n",
    "\n",
    "# #     clear_output()\n",
    "    plt.plot(train_iterations, train_losses, 'b',val_iterations, val_losses, 'r')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss') \n",
    "    plt.legend(['Train','Val'])\n",
    "    plt.title('Epoch vs Loss')\n",
    "#     plt.show()\n",
    "    plt.savefig(\"./part_seg_losses.png\") # save graph for training visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###save stuff\n",
    "folder = 'results/'\n",
    "torch.save(net.state_dict(), folder+str(class_ID)+'_part_seg_model')\n",
    "filehandler = open(folder+str(class_ID)+'_part_seg_train_iterations', 'wb') \n",
    "pickle.dump(train_iterations, filehandler)\n",
    "filehandler = open(folder+str(class_ID)+'_part_seg_train_losses', 'wb') \n",
    "pickle.dump(train_losses, filehandler)\n",
    "filehandler = open(folder+str(class_ID)+'_part_seg_val_iterations', 'wb') \n",
    "pickle.dump(val_iterations, filehandler)\n",
    "filehandler = open(folder+str(class_ID)+'_part_seg_val_losses', 'wb') \n",
    "pickle.dump(val_losses, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot training loss\n",
    "import pickle\n",
    "filehandler = open(folder+str(class_ID)+'_part_seg_train_iterations', 'rb') \n",
    "train_iterations = pickle.load(filehandler)\n",
    "filehandler = open(folder+str(class_ID)+'_part_seg_train_losses', 'rb') \n",
    "train_losses = pickle.load(filehandler)\n",
    "filehandler = open(folder+str(class_ID)+'_part_seg_val_iterations', 'rb') \n",
    "test_iterations = pickle.load(filehandler)\n",
    "filehandler = open(folder+str(class_ID)+'_part_seg_val_losses', 'rb') \n",
    "test_losses = pickle.load(filehandler)\n",
    "\n",
    "import os,math,numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate\n",
    "\n",
    "matplotlib.rcParams['axes.linewidth'] = 5\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "plt.rc('font',size=35)\n",
    "\n",
    "iter=train_iterations\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "fig.set_size_inches(12*1.5,8*1.5)\n",
    "xnew=np.linspace(min(iter),max(iter),200)  \n",
    "test_loss_spline=scipy.interpolate.CubicSpline(iter,test_losses)\n",
    "test_data = test_loss_spline(xnew)\n",
    "\n",
    "ln=ax.plot(xnew,test_data,label=\"Validation Loss\",linewidth=5,color='#ff7f0e')\n",
    "ax.tick_params(axis='y',colors='#ff7f0e')\n",
    "\n",
    "ax2=ax.twinx()\n",
    "xnew=np.linspace(min(iter),max(iter),200)  \n",
    "train_loss_spline=scipy.interpolate.CubicSpline(iter,train_losses)\n",
    "train_data = train_loss_spline(xnew)\n",
    "ln+=ax2.plot(xnew,train_data,label=\"Train Loss\",linewidth=5,color='#1f77b4')\n",
    "ax2.tick_params(axis='y',colors='#1f77b4')\n",
    "ax2.spines['right'].set_color('#1f77b4')\n",
    "ax2.spines['left'].set_color('#ff7f0e')\n",
    "\n",
    "labs=[l.get_label() for l in ln]\n",
    "ax.legend(ln,labs,loc=0)\n",
    "\n",
    "#ax.set_ylabel(\"Value\")\n",
    "ax.set_xlabel(\"#Iteration\")\n",
    "plt.savefig(\"Iteration.pdf\",bbox_inches='tight',pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute accuracy for the test set \n",
    "\n",
    "def get_all_data(dataset,data_type):\n",
    "    pcds,labels,fns = dataset.get_all_data(data_type)\n",
    "    return pcds,labels\n",
    "\n",
    "all_data,all_labels = get_all_data(ShapeNetData,'test')\n",
    "mIoU = 0.\n",
    "for data,labels in zip(all_data,all_labels):\n",
    "    data = torch.from_numpy(np.expand_dims(np.array(data),0)).float().to(device)\n",
    "    labels = torch.from_numpy(np.expand_dims(np.array(labels),0)).to(device)\n",
    "\n",
    "    #compute the loss\n",
    "    preds,M2 = net(data)\n",
    "    loss = criterion(preds[0,:,:],labels[0,:])\n",
    "\n",
    "    #add transformation matrix regularization loss\n",
    "    I = torch.eye(64).unsqueeze(0).to(device)\n",
    "    loss2 = torch.mean(torch.norm(torch.bmm(M2,M2.transpose(2,1)) - I, dim=(1,2)))    \n",
    "    loss += REG_WEIGHT*loss2\n",
    "\n",
    "    mIoU_one_part = compute_mIoU(torch.max(preds[0,:,:],dim = 1).values,labels[0,:])\n",
    "    mIoU +=mIoU_one_part\n",
    "    \n",
    "print('test mIoU:',mIoU/len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
